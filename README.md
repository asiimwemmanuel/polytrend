<!-- print linux branch onto master, delete all app dev branches. -->
<!-- next part of development: register on PyPI, make docker image, generate binaries -->

<!-- # PolyTrend üìà -->
<p align="center">
    <img src="./assets/images/logo.jpg" alt="PolyTrend logo" width="400">
</p>
<!-- ![Cat](./assets/images/logo.jpg) -->

## Introduction üí°

PolyTrend is a Python app that approximates and plots a polynomial function onto given data. Its development provides insights and conclusions in the fields of interpolation, polynomial regression and approximation theory.

<div style="display:flex; justify-content:center;">
    <div style="flex:1; text-align:center;">
        <img src="./assets/images/ui.png" alt="PolyTrend UI screenshot" width="400">
    </div>
    <div style="flex:1; text-align:center;">
        <img src="./assets/images/starter.png" alt="PolyTrend graph screenshot" width="400">
    </div>
</div>

## Installation ‚öôÔ∏è

<!-- methods to include: source (suitable for modification/customization), python package index (suitable for usage in projects), docker image (suitable for running in a VM), installation wizard with binary(suitable for standalone releases) -->

### From release page

To install PolyTrend from the release page with included instructions and ensure compatibility with the specified Python version, follow these steps:

1. Visit the [PolyTrend release page](https://github.com/asiimwemmanuel/polytrend/releases) and download the desired release package (ZIP or GZ file).

2. Extract the downloaded package to a directory of your choice.

3. Navigate to the extracted directory in your file explorer.

4. Locate the installation instructions file included in the extracted package. This file typically contains detailed instructions on how to set up and run PolyTrend.

5. Follow the instructions provided in the file to install and run PolyTrend. Make sure to pay attention to any specific requirements or recommendations mentioned in the instructions.

### From release page

To install PolyTrend from the release page with included instructions and ensure compatibility with the specified Python version, follow these steps:

1. Visit the [PolyTrend release page](https://github.com/asiimwemmanuel/polytrend/releases) and download the desired release package (ZIP or GZ file).

2. Extract the downloaded package to a directory of your choice.

3. Navigate to the extracted directory in your file explorer.

4. Locate the installation instructions file included in the extracted package. This file typically contains detailed instructions on how to set up and run PolyTrend.

5. Follow the instructions provided in the file to install and run PolyTrend. Make sure to pay attention to any specific requirements or recommendations mentioned in the instructions.

### From source

To install PolyTrend **from source** and ensure the correct Python version using a virtual environment, follow these steps:
To install PolyTrend **from source** and ensure the correct Python version using a virtual environment, follow these steps:

#### 1. Install [Python](https://www.python.org/downloads/) and [Git](https://git-scm.com/downloads) if not already installed on your system.

#### 2. Install `freeglut3-dev`

> - **macOS (brew)**:
>```bash
>    brew install freeglut
>```
> - **Windows (chocolatey)**:
>```bash
>    choco install freeglut
>```
> - **Debian/Ubuntu (apt)**:
>```bash
>    sudo apt-get install freeglut3-dev
>```
> 
> - **Fedora (dnf)**:
>```bash
>    sudo dnf install freeglut-devel
>```
> 
> - **Red Hat/CentOS (yum)**:
>```bash
>    sudo yum install freeglut-devel
>```
> 
> - **Arch Linux (pacman)**:
>```bash
>    sudo pacman -S freeglut
>```
> 
> - **openSUSE (zypper)**:
>```bash
>    sudo zypper install freeglut-devel
>```

#### 3. Set up the virtual environment (via [pyenv](https://github.com/pyenv/pyenv)):

```shell
git clone https://github.com/asiimwemmanuel/polytrend.git
cd polytrend
pyenv virtualenv venv_name # or python -m venv venv_name
```

Replace `venv_name` with the desired name for your virtual environment.

#### 4. Activate the virtual environment:

- On **Windows**:

```shell
venv_name\Scripts\activate
```

- On **Unix or MacOS**:

```shell
pyenv activate venv_name # or source venv_name/bin/activate
```

#### 5. Verify and set the Python version specified in `.python-version`:

```shell
pyenv local $(cat .python-version)
```

#### 6. Install required dependencies:

```shell
pyenv virtualenv venv_name # or python -m venv venv_name
```

Replace `venv_name` with the desired name for your virtual environment.

#### 4. Activate the virtual environment:

- On **Windows**:

```shell
venv_name\Scripts\activate
```

- On **Unix or MacOS**:

```shell
pyenv activate venv_name # or source venv_name/bin/activate
```

#### 5. Verify and set the Python version specified in `.python-version`:

```shell
pyenv local $(cat .python-version)
```

#### 6. Install required dependencies:

```shell
pip install -r requirements.txt
```


This command sets the Python version specified in `.python-version` as the local version for the current directory. Ensure you have `pyenv` installed for this command to work.

#### 7. Run the app:

This command sets the Python version specified in `.python-version` as the local version for the current directory. Ensure you have `pyenv` installed for this command to work.

#### 7. Run the app:

```shell
python main.py
python main.py
```

> Note for linux: If issues persist, visit [this website](https://web.stanford.edu/dept/cs_edu/resources/qt/install-linux) and follow the instructions to configure Qt on your system

<!-- ### From the python packaging index * -->

<!-- To install polytrend **as a package**, follow these instructions:

- Install [Python](https://www.python.org/downloads/) if it is not already installed on your system. üíª

- Install the package:

```shell
pip install polytrend
```
-->

<!-- ### From a docker image

This will soon be available... -->

<!-- ### From a standalone release/binary

To install polytrend via a wizard, see [Releases](https://github.com/asiimwemmanuel/polytrend/releases)-->

<!-- Note: If you are using WSL or a GUI incapable OS, install an X11 server like vcxsrv and link your forwarding instance with your terminal session.
Do so only if the app fails to run in case you installed it from source -->

## Code Explanation: `PolyTrend` Class for Polynomial Trend Analysis üßÆ

The code defines a class called `PolyTrend` that provides methods for polynomial trend analysis. Here's a summary of the code structure and its functionalities:

### Class Definition

The `PolyTrend` class encapsulates methods for polynomial trend analysis.

### Method: `polyplot`
### Method: `polyplot`

This method plots the best polynomial fit for known data points. It takes the following parameters:

- `degrees`: A list of polynomial degrees to consider.
- `known_data`: A list of tuples representing the known data points.
- `extrap_data` (optional): A list of x coordinates for extrapolation.

### Method: `polyfind`
### Method: `polyfind`

This method finds the best-fit polynomial function. It takes the following parameters:

- `degrees`: A list of polynomial degrees to consider.
- `known_data`: A list of tuples representing the known data points.
- It returns a callable function that predicts values based on the polynomial.

### Method: `polygraph`
### Method: `polygraph`

This method plots the function, known data points, and extrapolated data points. It takes the following parameters:

- `known_data`: A list of tuples representing the known data points.
- `extrap_data` (optional): A list of extrapolation data points.
- `func` (optional): A function to generate predicted values.

### Error Handling
### Error Handling

The code includes error handling to raise a `ValueError` if required parameters are missing or invalid.

## Theory üîçüî¢

### Quadratic Sequences

If a quadratic sequence follows the form:

$a, b, c$

And while letting;

- $\alpha = b - a$
- $\beta = c - b$
- $x = \beta - \alpha$

It can be observed that $\alpha$ and $\beta$ form a linear sequence, represented as:

```math
    xn + (\alpha - x)
```

or

```math
    \alpha + \sum_{i=1}^{n-1}x
```

Taking the bottommost layer (x) of the difference table to have a degree of 0 i.e constant, the linear sequence can be seen as a summation of $x_i$ and the first term. After all, finding the nth term at any degree involves regressive computation of the underlying layers.
With this same logic, quadratic sequences (and any other, for that matter) can be computed as their first term $a$ summed with a certain combination of underlying variables.

Asiimwe's general form of quadratic nth term problems:

```math
    a + \sum_{i=1}^{n-1}\alpha + x(i-1)
```

<!-- also comment about how this ties into Sequences and series, and general formulae for calculating summations -->

The above formula is limited to linear and quadratic sequences, requiring a different method for higher degrees. Notice the general form of linear sequences as the subject of summation. It can be inferred that that expression itself is the subject of summation for cubic problems and THAT expression is the subject of quartic problems. A recursive pattern is observed in this approach when generalizing for problems with variable degrees.

It's also interesting to think about traversing the layers downwards as differentiation and upwards as integration.

**HYPOTHESIS**: when the sequence is represented with a polynomial, the $f^{(d)}$ derivative is a constant equating the bottommost layer.

My attempts to use this property to build `quadseq` via Calculus thereby forming a general method for all degree problems fell short as attempting to integrate (traverse upwards) from the lower layers led to the loss of information (constants and extra terms), causing a transformation in the plots of the main sequence and the integrated one.

My untested hypothesis to regain this information so far is either to look at the full main and integrated sequences (while noticing one is a transformation of the other) and find the directed (that is, not absolute) phase in each sequence term or find such a phase at each integral of the process, rather at the end; both help in finding the missing information. This is best to deduce missing polynomial constants and is more challenging for other types of lost function terms.

<!-- TODO: include an example of the hypothesis -->

### Lagrange Interpolation

Lagrange interpolation is a method primarily used for in-bound approximation. However, its properties make it suitable for nth-term problems as they are without error.

The Lagrange interpolation formula is given by:

```math
    P(x)=\sum_{i=0}^{n}\left(y_i\prod_{j=0,j\neq i}^{n}\frac{x-x_j}{x_i-x_j}\right)
```

Where:

- $P(x)$ represents the polynomial of degree $n$ (where $n$ is the number of data points). <!-- ! check for error... -->
- $y_i$ denotes the y-coordinate of the $i^{th}$ data point.
- $x_i$ represents the x-coordinate of the $i^{th}$ data point.

This formula calculates the lowest order polynomial $P(x)$ that passes through the given data points $x_i, y_i$. It uses a weighted sum of Lagrange basis polynomials to interpolate the function or estimate the

 value at a specific point $x$, where $x$ represents $n$.

**Note**: $n$ in the formula is not the same as the polynomial degree $q$ used in the previous section.

> For more information on the Lagrange polynomial, refer to the [Lagrange polynomial Wiki](https://en.wikipedia.org/wiki/Lagrange_polynomial).

To increase error tolerance and applicability in real-world data, a new technique is incorporated.

### Polynomial Regression

talk about how matrices solve the least squares problem and how matrices are really applicable for a variety of problems beyong this one

talk about how matrices solve the least squares problem and how matrices are really applicable for a variety of problems beyong this one

Polynomial regression is used to shift from discrete to continuous data, making it applicable in real-world data analytics where error is present. It offers a tradeoff between accuracy and generality for various applications.

Given a set of $n$ data points $(x, f(x))$, the polynomial function of degree $m$ is approximated as:

```math
     f(x_i) = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \cdots + \beta_m x_i^m + \varepsilon_i \quad \text{(for i = 1, 2, \ldots, n\text)} 
```

Here, $m$ signifies the chosen maximum power, each $\beta_i$ represents a coefficient in the function, and $\varepsilon_i$ denotes random error.

The approximation is determined by solving for $\vec{\beta}$ in the matrix equation:

```math

    \begin{bmatrix}
        1 & x_0 & x_0^2 & \dots & x_0^m \\
        1 & x_1 & x_1^2 & \dots & x_1^m \\
        1 & x_2 & x_2^2 & \dots & x_2^m \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        1 & x_n & x_n^2 & \dots & x_n^m
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
        \beta_0\\
        \beta_1\\
        \beta_2\\
        \vdots \\
        \beta_m
    \end{bmatrix}
    +
    \begin{bmatrix}
        \varepsilon_0\\
        \varepsilon_1\\
        \varepsilon_2 \\
        \vdots \\
        \varepsilon_n
    \end{bmatrix}
    =
    \begin{bmatrix}
        y_0 \\
        y_1 \\
        y_2 \\
        \vdots \\
        y_n
    \end{bmatrix}

```

The matrix equation $ \mathbf{X} \vec{\beta} + \vec{\varepsilon} = \vec{y}$ is solved using the [Normal Equation](http://mlwiki.org/index.php/Normal_Equation):

```math
    \widehat{\vec{\beta}} = (\mathbf{X}^\mathsf{T} \mathbf{X})^{-1} \mathbf{X}^\mathsf{T} \vec{y} 
```

Where:

- $\widehat{\vec{\beta}}$ is the estimated coefficient vector.
- $\mathbf{X}$ is the matrix of input features.
- $\vec{y}$ is the vector of target values.
- $\mathbf{X}^\mathsf{T}$ denotes the transpose of the input feature matrix.
- $(\mathbf{X}^\mathsf{T} \mathbf{X})^{-1}$ represents the inverse of the covariance matrix of the input features.

Note that the product of the transpose and inverse of the covariance of $ \mathbf{X}$ is also termed the [pseudoinverse](http://mlwiki.org/index.php?title=General_Inverse&action=edit&redlink=1) $\mathbf{X^{+}}$, thus the coefficient vector is the product of the pseudoinverse and the target vector.

Dimensions of the matrices:

- $\mathbf{X}$ is $(n+1)$ by $(m+1)$,
- $\widehat{\vec{\beta}}$ is $(m+1)$ by $1$,
- $\vec{y}$ is $(n+1)$ by $1$.

> For further information on polynomial regression, refer to the [Polynomial regression Wiki](https://en.wikipedia.org/wiki/Polynomial_regression)

### Model Selection

Throughout my implementation of polynomial regression methods in generating predictive models, I noticed a peculiar tendency; given the task of choosing the degree most optimal for the model, say between $a$ and $b$, the algorithm always chose $max(a, b)$. This made sense since the algorithm judged the better degree to be that whose polynomial achieved a higher score whose metric I simply took to be MSE.

I am convinced this was a notorious problem at least offhead by Occam's Razor (that the simpler answer is often more correct), by the fact that programmers would likely refine their algorithms to cut out needless calculation, and also by the fact that there were already multiple responses to it, the majority of which resembled (as they tend to in such fields).

The Bayesian Information Criterion (BIC) is the most well known, and states:

```math
     BIC = n \cdot \ln(MSE) + k \cdot \ln(n) 
```

Where:

- $ n $ is the number of data points.
- $ MSE $ is the mean squared error.
- $ k $ is the number of parameters in the regression model.

```math
     MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 
```

Where:

- $ n $ is the number of data points.
- $ y_i $ is the actual value for the $i$-th data point.
- $ \hat{y}_i $ is the predicted value for the $i$-th data point.

The beauty of BIC is while it rewards accuracy (the first term in the summation), it also penelizes complexity in its second term. What I find peculiar is it's additive rather than multiplicative as we've observed with numerous scientific derived quantities (one example of which is velocity). This intrigued me to investigate its derivation.

> For more info, see [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion)

<!-- find a way to incorporate hypothesis testing, specifically to reject the null hypothesis, and maybe tie connections with linear algebra -->

## Future Improvements üîÆüîß

üöÄ **Possible Improvements for the Script:**

1. **Batch Plotting:** üîÑ Create a separate function to perform polynomial regression and plot the results, to avoid redundant computations.

2. **Caching:** üóÑÔ∏è Implement caching to store regression results for different degrees and known data, speeding up repeated calculations.

3. **Data Normalization:** üìè Normalize the data before regression to ensure consistent scaling and improve model convergence.

4. **Regularization:** üéõÔ∏è Add regularization terms (L1/Lasso or L2/Ridge) to the regression model to prevent overfitting.

5. **Model Evaluation:** üìä Consider additional metrics (e.g., MAE, MSE) for a comprehensive assessment of model performance.

6. **Error Handling:** ‚ùó Enhance error messages and gracefully handle edge cases to improve script robustness.

7. **Optimize Graph Prerequisites:** üìà Fine-tune graph settings for better visualization and aesthetics.

8. **Parallelization:** üèãÔ∏è‚Äç‚ôÄÔ∏è Explore parallelization to distribute computational load for large datasets.

9. **Outlier Handling:** üö® Address potential outliers in the data for more robust regression results.

10. **Feature Selection:** üéØ Investigate feature selection techniques to identify the most relevant predictors.

11. **Cross-Validation:** üîÑ Implement cross-validation to assess model generalization and performance.

12. **Multivariate Regression:** üßÆ Extend the script for multivariate regression scenarios.

13. **Data Preprocessing:** üõ†Ô∏è Improve data preprocessing techniques for cleaner data inputs.

14. **Documentation:** üìù Enhance code comments and provide detailed documentation for future maintainability.

15. **Performance Profiling:** üõ†Ô∏è Use tools like `cProfile` to identify performance bottlenecks and optimize critical sections.

16. **Model Selection:** ü§î Compare different regression models (e.g., polynomial, linear, etc.) for the best fit.

17. **Feature Engineering:** üîß Explore feature engineering to create more informative predictors.

18. **Visualizations:** üìä Create additional informative visualizations for data exploration and model evaluation.

19. **Ensemble Models:** ü§ù Consider ensemble methods (e.g., stacking, bagging) to improve prediction accuracy.

20. **Hyperparameter Tuning:** üîç Fine-tune model hyperparameters to optimize performance.

The relevance of these improvements depends on the specific use case and data characteristics. Prioritize the changes based on the requirements and constraints of your problem to achieve the best possible results. üåü

Feel free to add to this repo with your specialized tools according to your preference and expertise üòâ üöÄ
